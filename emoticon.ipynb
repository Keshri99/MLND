{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Facial emotion recognition\n",
    "\n",
    "---\n",
    "\n",
    "## Step 0: Prepare image repository\n",
    "\n",
    "### Generate images from pixels\n",
    "\n",
    "We begin with generating images from pixel dataset and storing them in their respective folders according thier emotion class and category (Train, PrivateTest, PublicTest). \n",
    "The library `PIL` is used to convert pixels to images. \n",
    "- `fer2013.csv` - base dataset hosted by the Kaggle competition\n",
    "\n",
    "Each row represents a face. There are 3 columns:\n",
    "- `emotions` - emotion expressed by the face\n",
    "- `pixels` - this column contains 2304 pixel values in space-separated manner\n",
    "- `category` - purpose of the image (Train, PrivateTest, PublicTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc as smp\n",
    "import csv\n",
    "from PIL import Image\n",
    "count = 1\n",
    "strEmotion = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c74e2bd4ca71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('fer/fer2013.csv', 'rt') as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for row in spamreader:\n",
    "        x = row[1].split()\n",
    "        x1=np.reshape(x,(48,48))\n",
    "        img = Image.new( 'RGB', (48,48), \"black\") \n",
    "        pixels = img.load() # create the pixel map\n",
    "        for i in range(img.size[0]):    # for every col:\n",
    "            for j in range(img.size[1]):    # For every row\n",
    "                pixels[i,j] = (int(x1[j][i]), 0, 0) \n",
    "\n",
    "        img.save('fer/'+row[2]+'/'+row[0]+'_'+strEmotion[int(row[0])]+'/'+row[0] +'_'+str(count)+'.jpg')\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Import image data\n",
    "\n",
    "### Read images as 4D tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d569361b082f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    emo_files = np.array(data['filenames'])\n",
    "    emo_targets = np_utils.to_categorical(np.array(data['target']), 7)\n",
    "    return emo_files, emo_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('fer/Training')\n",
    "valid_files, valid_targets = load_dataset('fer/PrivateTest')\n",
    "test_files, test_targets = load_dataset('fer/PublicTest')\n",
    "\n",
    "# load list of emotions\n",
    "emo_names = [item[13:-1] for item in sorted(glob(\"fer/Training/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total emotion categories.' % len(emo_names))\n",
    "print('There are %s total facial images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training facial images.' % len(train_files))\n",
    "print('There are %d validation facial images.' % len(valid_files))\n",
    "print('There are %d test facial images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path,grayscale=True, target_size=(48, 48))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 2: Rescale the image \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 28709/28709 [00:14<00:00, 2002.79it/s]\n",
      "100%|████████████████████████████████████| 3589/3589 [00:01<00:00, 2022.54it/s]\n",
      "100%|████████████████████████████████████| 3589/3589 [00:01<00:00, 1968.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 3: Model Architecture\n",
    "\n",
    "The code cell below defines the model architecture: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\A\\envs\\emoticon\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Conv2D)               (None, 48, 48, 4)         68        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 48, 4)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 8)         1160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 24, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 16)        8208      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                14425     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                390       \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 7)                 112       \n",
      "=================================================================\n",
      "Total params: 24,363.0\n",
      "Trainable params: 24,363.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D,GlobalMaxPooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import TruncatedNormal,glorot_normal\n",
    "import math\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "n = 0.01\n",
    "model.add(Conv2D(filters=4, kernel_size=4, strides=1, padding='same', activation='relu',input_shape=(48, 48, 1)\n",
    "                 ,kernel_initializer=glorot_normal(seed=0)\n",
    "                 ,bias_initializer='zeros',name = \"Input\"))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "n = 0.01\n",
    "model.add(Conv2D(filters=8, kernel_size=6, strides=1, padding='same', activation='relu'\n",
    "                 ,kernel_initializer=glorot_normal(seed=0)\n",
    "                 ,bias_initializer='zeros'))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=8, strides=1, padding='same', activation='relu'\n",
    "                 ,kernel_initializer=glorot_normal(seed=0)\n",
    "                 ,bias_initializer='zeros'))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(25,activation = 'relu'))\n",
    "model.add(Dense(15,activation = 'relu'))\n",
    "#n = math.sqrt(2.0/(9*256))\n",
    "model.add(Dense(7,activation='softmax',name=\"Output\"))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\A\\envs\\emoticon\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2548: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\A\\envs\\emoticon\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 1.80231, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00001: val_loss improved from 1.80231 to 1.77558, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00002: val_loss improved from 1.77558 to 1.71352, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00003: val_loss improved from 1.71352 to 1.68931, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00004: val_loss improved from 1.68931 to 1.64174, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00005: val_loss improved from 1.64174 to 1.59286, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00007: val_loss improved from 1.59286 to 1.57016, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00011: val_loss improved from 1.57016 to 1.53976, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00012: val_loss improved from 1.53976 to 1.51542, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00014: val_loss improved from 1.51542 to 1.51234, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00016: val_loss improved from 1.51234 to 1.50532, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 00019: val_loss improved from 1.50532 to 1.50056, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 00022: val_loss improved from 1.50056 to 1.45190, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 00028: val_loss improved from 1.45190 to 1.42306, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 00030: val_loss improved from 1.42306 to 1.42115, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 00043: val_loss improved from 1.42115 to 1.41121, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 00052: val_loss improved from 1.41121 to 1.39885, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 00065: val_loss improved from 1.39885 to 1.37788, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 00089: val_loss improved from 1.37788 to 1.36674, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 00099: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "\n",
    "epochs = 100\n",
    "csv_logger = CSVLogger('logs/training.csv')\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hist = model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=70, callbacks=[checkpointer,csv_logger], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 4: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 48.3422%\n"
     ]
    }
   ],
   "source": [
    "emotion_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(emotion_predictions)==np.argmax(test_targets, axis=1))/len(emotion_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 5: Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline\n",
    "strEmotion = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
    "def get_emotion(img_path):\n",
    "    emotion_class = model.predict_classes(path_to_tensor(img_path))\n",
    "    return emotion_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def emotion_detector(img_path):\n",
    "    emotion = get_emotion(img_path) \n",
    "    img = cv2.imread(img_path)\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(cv_rgb)\n",
    "    plt.show()\n",
    "    print(\"Emotion detected: \" + str(strEmotion[emotion]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d89a828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9e3bcb16b758>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(test_imgs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_imgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0memotion_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Actual emotion: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrEmotion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-48a1c70d3b68>\u001b[0m in \u001b[0;36memotion_detector\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_rgb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Emotion detected: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrEmotion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0memotion\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "test_imgs = np.array(glob(\"test_images/*\"))\n",
    "#print(test_imgs)\n",
    "for img in test_imgs:\n",
    "    emotion_detector(img)\n",
    "    print(\"Actual emotion: \" + str(strEmotion[int(img[12:13])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\A\\envs\\emoticon\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_model(saver, model, input_node_names, output_node_name):\n",
    "    tf.train.write_graph(K.get_session().graph_def, 'out_kn', \\\n",
    "        MODEL_NAME + '_graph.pbtxt')\n",
    "\n",
    "    saver.save(K.get_session(), 'out_kn/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "    freeze_graph.freeze_graph('out_kn/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        False, 'out_kn/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "        'out_kn/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out_kn/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('out_kn/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from out_kn/emoticon.chkp\n",
      "INFO:tensorflow:Froze 12 variables.\n",
      "Converted 12 variables to const ops.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The following input nodes were not found: {'Input/truncated_normal'}\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-bf2b3a9ecd6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'emoticon'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mexport_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Input/truncated_normal\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Output/Softmax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-1e070f9d464d>\u001b[0m in \u001b[0;36mexport_model\u001b[1;34m(saver, model, input_node_names, output_node_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m     output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n\u001b[0;32m     13\u001b[0m             \u001b[0minput_graph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_node_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moutput_node_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             tf.float32.as_datatype_enum)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFastGFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'out_kn/opt_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\A\\envs\\emoticon\\lib\\site-packages\\tensorflow\\python\\tools\\optimize_for_inference_lib.py\u001b[0m in \u001b[0;36moptimize_for_inference\u001b[1;34m(input_graph_def, input_node_names, output_node_names, placeholder_type_enum)\u001b[0m\n\u001b[0;32m    107\u001b[0m   optimized_graph_def = strip_unused_lib.strip_unused(\n\u001b[0;32m    108\u001b[0m       \u001b[0moptimized_graph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_node_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_node_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m       placeholder_type_enum)\n\u001b[0m\u001b[0;32m    110\u001b[0m   optimized_graph_def = graph_util.remove_training_nodes(\n\u001b[0;32m    111\u001b[0m       optimized_graph_def, output_node_names)\n",
      "\u001b[1;32mC:\\A\\envs\\emoticon\\lib\\site-packages\\tensorflow\\python\\tools\\strip_unused_lib.py\u001b[0m in \u001b[0;36mstrip_unused\u001b[1;34m(input_graph_def, input_node_names, output_node_names, placeholder_type_enum)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnot_found\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The following input nodes were not found: %s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnot_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m   output_graph_def = graph_util.extract_sub_graph(inputs_replaced_graph_def,\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The following input nodes were not found: {'Input/truncated_normal'}\\n\""
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'emoticon'\n",
    "export_model(tf.train.Saver(), model, [\"Input/truncated_normal\"], \"Output/Softmax\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
