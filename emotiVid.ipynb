{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 0: Prepare image repository\n",
    "\n",
    "### Generate images from pixels\n",
    "\n",
    "We begin with generating images from pixel dataset and storing them in their respective folders according thier emotion class and category (Train, PrivateTest, PublicTest). \n",
    "The library `PIL` is used to convert pixels to images. \n",
    "- `fer2013.csv` - base dataset hosted by the Kaggle competition\n",
    "\n",
    "Each row represents a face. There are 3 columns:\n",
    "- `emotions` - emotion expressed by the face\n",
    "- `pixels` - this column contains 2304 pixel values in space-separated manner\n",
    "- `category` - purpose of the image (Train, PrivateTest, PublicTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc as smp\n",
    "import csv\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "count = 1\n",
    "strEmotion = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('fer/fer2013.csv', 'rt') as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for row in spamreader:\n",
    "        x = row[1].split()\n",
    "        x1=np.reshape(x,(48,48))\n",
    "        img = Image.new( 'RGB', (48,48), \"black\") \n",
    "        pixels = img.load() # create the pixel map\n",
    "        for i in range(img.size[0]):    # for every col:\n",
    "            for j in range(img.size[1]):    # For every row\n",
    "                pixels[i,j] = (int(x1[j][i]), 0, 0) \n",
    "\n",
    "        img.save('fer/'+row[2]+'/'+row[0]+'_'+strEmotion[int(row[0])]+'/'+row[0] +'_'+str(count)+'.jpg')\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Import image data\n",
    "\n",
    "### Read images as 4D tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    emo_files = np.array(data['filenames'])\n",
    "    emo_targets = np_utils.to_categorical(np.array(data['target']), 7)\n",
    "    return emo_files, emo_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('fer/Training')\n",
    "valid_files, valid_targets = load_dataset('fer/PrivateTest')\n",
    "test_files, test_targets = load_dataset('fer/PublicTest')\n",
    "\n",
    "# load list of emotions\n",
    "emo_names = [item[13:-1] for item in sorted(glob(\"fer/Training/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total emotion categories.' % len(emo_names))\n",
    "print('There are %s total facial images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training facial images.' % len(train_files))\n",
    "print('There are %d validation facial images.' % len(valid_files))\n",
    "print('There are %d test facial images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path,grayscale=True, target_size=(48, 48))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_path = 'test_images/6_32609.jpg'\n",
    "zz = path_to_tensor(img_path)\n",
    "shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 2: Rescale the image \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.merge_ops import merge_outputs, merge\n",
    "from tflearn.layers.normalization import local_response_normalization, batch_normalization\n",
    "from tflearn.layers.estimator import regression \n",
    "from tflearn.optimizers import Momentum, Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 3: Model Architecture\n",
    "\n",
    "The code cell below defines the model architecture: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D,GlobalMaxPooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import TruncatedNormal,glorot_normal\n",
    "import math\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "n = 0.01\n",
    "model.add(Conv2D(filters=32, kernel_size=5, strides=1, padding='same', activation='relu',input_shape=(48, 48, 1)\n",
    "                 ,kernel_initializer=glorot_normal(seed=0)\n",
    "                 ,bias_initializer='zeros',name = \"Input\"))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(MaxPooling2D(pool_size=3, strides=2))\n",
    "\n",
    "n = 0.01\n",
    "model.add(Conv2D(filters=32, kernel_size=5, strides=1, padding='same', activation='relu'\n",
    "                 ,kernel_initializer=glorot_normal(seed=0)\n",
    "                 ,bias_initializer='zeros'))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(MaxPooling2D(pool_size=3, strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=4, strides=1, padding='same', activation='relu'\n",
    "                 ,kernel_initializer=glorot_normal(seed=0)\n",
    "                 ,bias_initializer='zeros'))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "#model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100,activation = 'relu'))\n",
    "#model.add(Dense(15,activation = 'relu'))\n",
    "#n = math.sqrt(2.0/(9*256))\n",
    "model.add(Dense(7,activation='softmax',name=\"Output\"))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "\n",
    "epochs = 20\n",
    "csv_logger = CSVLogger('logs/training.csv')\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.tudelft.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hist = model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=50, callbacks=[checkpointer,csv_logger], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.tudelft.hdf5')\n",
    "model.save('saved_models/model_tudelft.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_saved = load_model('saved_models/model_tudelft.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 4: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emotion_predictions = [np.argmax(model_saved.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(emotion_predictions)==np.argmax(test_targets, axis=1))/len(emotion_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 5: Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline\n",
    "strEmotion = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
    "def get_emotion(img_path):\n",
    "    emotion_class = model_saved.predict_classes(path_to_tensor(img_path))\n",
    "    return emotion_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def emotion_detector(img_path):\n",
    "    emotion = get_emotion(img_path) \n",
    "    img = cv2.imread(img_path)\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(cv_rgb)\n",
    "    plt.show()\n",
    "    print(\"Emotion detected: \" + str(strEmotion[np.asscalar(emotion)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emotion_detector('test_images/6_32609.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emotion_probs = model_saved.predict_classes(path_to_tensor('test_images/0_32611.jpg'))\n",
    "emotion_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from cv2 import __version__\n",
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "json_string = 'C:/Work/1. Coursera/4. MLND/machine-learning-master/machine-learning-master/projects/dog-project-master/to/tensorflow-101-master/model/facial_expression_model_structure.json'\n",
    "#model = model_from_json(json_string)\n",
    "\n",
    "\n",
    "model_file = open('C:/Work/1. Coursera/4. MLND/machine-learning-master/machine-learning-master/projects/dog-project-master/to/tensorflow-101-master/model/facial_expression_model_structure.json', 'r')\n",
    "model = model_file.read()\n",
    "model_file.close()\n",
    "model_trans = model_from_json(model)\n",
    "model_trans.load_weights(\"C:/Work/1. Coursera/4. MLND/machine-learning-master/machine-learning-master/projects/dog-project-master/to/tensorflow-101-master/model/facial_expression_model_weights.h5\")\n",
    "model_trans.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "def frame_to_tensor(frm):\n",
    "    img = cv2.resize(frm, (48,48), interpolation = cv2.INTER_AREA)\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    x = image.img_to_array(gray_image)\n",
    "    #res = np.concatenate([x[np.newaxis]])\n",
    "    return np.expand_dims(x, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "strEmotion = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
    "def get_emotion_frame(frm):\n",
    "    emotion_class = model_saved.predict_classes(frame_to_tensor(frm))\n",
    "    emotion = str(strEmotion[np.asscalar(emotion_class)])\n",
    "    return emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from cv2 import __version__\n",
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "#cap = cv2.VideoCapture(0)\n",
    "filepath = 'C:/Work/1. Coursera/4. MLND/machine-learning-master/machine-learning-master/projects/dog-project-master/to/emoticon/'\n",
    "cap = cv2.VideoCapture('Test.mp4')\n",
    "\n",
    "cascade = cv2.CascadeClassifier('C:/Work/1. Coursera/4. MLND/machine-learning-master/machine-learning-master/projects/dog-project-master/to/emoticon/haarcascades/haarcascade_frontalface_alt.xml')\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    " \n",
    "# Read until video is completed\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    " \n",
    "    #find faces\n",
    "    grayImage = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    body = cascade.detectMultiScale(\n",
    "        grayImage,\n",
    "        scaleFactor = 1.5,\n",
    "        minNeighbors = 5,\n",
    "        minSize = (30,30),\n",
    "        flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "    \n",
    "    emo = get_emotion_frame(frame)\n",
    "    \n",
    "    for (x,y,w,h) in body:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),4)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,emo,(x,y), font, .5,(255,255,255),2,cv2.LINE_AA)\n",
    "        #roi_gray = grayImage[y:y+h, x:x+w]\n",
    "        #roi_color = frame[y:y+h, x:x+w]\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame',frame)\n",
    " \n",
    "    # Press Q on keyboard to  exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "      break\n",
    " \n",
    "  # Break the loop\n",
    "  else: \n",
    "    break\n",
    " \n",
    "# When everything done, release the video capture object\n",
    "cap.release()\n",
    " \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "landmark_predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "def land2coords(landmarks, dtype=\"int\"):\n",
    "    # initialize the list of tuples\n",
    "    # (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    " \n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (a, b)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
    " \n",
    "    # return the list of (a, b)-coordinates\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    #import cv2\n",
    "    #from imutils import face_utils\n",
    "    import numpy as np\n",
    "    import argparse\n",
    "    import imutils\n",
    "    import dlib\n",
    "    import cv2\n",
    "    filepath = 'C:/Work/1. Coursera/4. MLND/machine-learning-master/machine-learning-master/projects/dog-project-master/to/emoticon'\n",
    "    vid = cv2.VideoCapture('test_vids/charlie.webm')\n",
    "    #vid = cv2.VideoCapture(filepath + 'Test.mp4')\n",
    "    #vid = cv2.VideoCapture(0)\n",
    " \n",
    "    if (vid.isOpened()== False): \n",
    "      print(\"Error opening video stream or file\")\n",
    "\n",
    "    while vid.isOpened():\n",
    "        _,frame = vid.read()\n",
    " \n",
    "        # resizing frame\n",
    "        # you can use cv2.resize but I recommend imutils because its easy to use\n",
    "        frame = imutils.resize(frame, width=800)\n",
    " \n",
    "        # grayscale conversion of image because it is computationally efficient\n",
    "        # to perform operations on single channeled (grayscale) image\n",
    "        frame_gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "        # detecting faces\n",
    "        face_boundaries = face_detector(frame_gray,0)\n",
    " \n",
    "        for (enum,face) in enumerate(face_boundaries):\n",
    "            # let's first draw a rectangle on the face portion of image\n",
    "            x = face.left()\n",
    "            y = face.top()\n",
    "            w = face.right() - x\n",
    "            h = face.bottom() - y\n",
    "            # Drawing Rectangle on face part\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (120,160,230),2)\n",
    " \n",
    "            # Now when we have our ROI(face area) let's\n",
    "            # predict and draw landmarks\n",
    "            landmarks = landmark_predictor(frame_gray, face)\n",
    "            # converting co-ordinates to NumPy array\n",
    "            landmarks = land2coords(landmarks)\n",
    "            for (a,b) in landmarks:\n",
    "                # Drawing points on face\n",
    "                cv2.circle(frame, (a, b), 2, (255, 0, 0), -1)\n",
    " \n",
    "            # Writing face number on image\n",
    "            #cv2.putText(frame, \"Face :{}\".format(enum + 1), (x - 10, y - 10),\n",
    "            #            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 128), 2)\n",
    "            emoz = frame_to_emo(frame)\n",
    "            cv2.putText(frame, \"Face :\" + str(emoz), (x - 10, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 128), 2)\n",
    " \n",
    "        cv2.imshow(\"frame\", frame)\n",
    " \n",
    "        #  Stop if 'q' is pressed\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break;\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "#img_path = 'test_images/mark.jpg'\n",
    "img_path = result['paths'][109]\n",
    "#img = image.load_img(img_path,grayscale=True)\n",
    "img = cv2.imread(img_path)\n",
    "img.shape\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "frame = imutils.resize(img, width=800)\n",
    "     \n",
    "        # grayscale conversion of image because it is computationally efficient\n",
    "        # to perform operations on single channeled (grayscale) image\n",
    "frame_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "        # detecting faces\n",
    "face_boundaries = face_detector(frame_gray,0)\n",
    " \n",
    "for (enum,face) in enumerate(face_boundaries):\n",
    "            # let's first draw a rectangle on the face portion of image\n",
    "    x = face.left()\n",
    "    y = face.top()\n",
    "    w = face.right() - x\n",
    "    h = face.bottom() - y\n",
    "            # Drawing Rectangle on face part\n",
    "    cv2.rectangle(frame, (x,y), (x+w, y+h), (120,160,230),2)\n",
    " \n",
    "            # Now when we have our ROI(face area) let's\n",
    "            # predict and draw landmarks\n",
    "    landmarks = landmark_predictor(frame_gray, face)\n",
    "            # converting co-ordinates to NumPy array\n",
    "    landmarks = land2coords(landmarks)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "import matplotlib.pyplot as plt\n",
    "#print(scaler.fit_transform(landmarks))\n",
    "#plt.plot([1,2,3,4])\n",
    "\n",
    "#plt.plot(df_new['lmarks'][1])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(scaler.fit_transform(df_new['lmarks'][1]))\n",
    "plt.show()\n",
    "scaler.fit_transform(df_new['lmarks'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D,GlobalMaxPooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import TruncatedNormal,glorot_normal\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    emo_files = np.array(data['filenames'])\n",
    "    emo_targets = np_utils.to_categorical(np.array(data['target']), 7)\n",
    "    return emo_files, emo_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "try:\n",
    "    train_files, train_targets = load_dataset('cohn-kanade')\n",
    "except:\n",
    "    pass\n",
    "#valid_files, valid_targets = load_dataset('fer/PrivateTest')\n",
    "#test_files, test_targets = load_dataset('fer/PublicTest')\n",
    "#emo_names = [item[13:-1] for item in sorted(glob(\"cohn-kanade/S010/*/\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(len(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_paths = glob(\"cohn-kanade/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print((data_paths[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "root = (\"cohn-kanade\")\n",
    "filepaths = []\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        filepaths.append(os.path.join(path, name))\n",
    "        #print (os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print((filepaths[0:10]))\n",
    "len(filepaths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_paths = pd.DataFrame.from_dict({'paths':filepaths})\n",
    "df_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_paths = pd.DataFrame.from_dict({'paths':filepaths})\n",
    "df_paths['key'] = df_paths.paths.str[:20]\n",
    "emomap = pd.read_csv(\"Cohn-Kanade Database FACS codes_v2.1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "root = (\"cohn-kanade\")\n",
    "filepaths = []\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        filepaths.append(os.path.join(path, name))\n",
    "        #print (os.path.join(path, name))\n",
    "df_paths = pd.DataFrame.from_dict({'paths':filepaths})\n",
    "df_paths['key'] = df_paths.paths.str[:20]\n",
    "emomap = pd.read_csv(\"Cohn-Kanade Database FACS codes_v2.1.csv\")\n",
    "result = pd.merge(df_paths, emomap, on='key', how='inner')\n",
    "#result['paths'][0]\n",
    "#result['paths']\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "#img_path = 'test_images/mark.jpg'\n",
    "def img_to_landmarks(imgpath):\n",
    "    img = cv2.imread(imgpath)\n",
    "    frame = imutils.resize(img, width=800)\n",
    "    frame_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    face_boundaries = face_detector(frame_gray,0)\n",
    "    for (enum,face) in enumerate(face_boundaries):\n",
    "        x = face.left()\n",
    "        y = face.top()\n",
    "        w = face.right() - x\n",
    "        h = face.bottom() - y\n",
    "        #cv2.rectangle(frame, (x,y), (x+w, y+h), (120,160,230),2)\n",
    "        landmarks = landmark_predictor(frame_gray, face)\n",
    "        landmarks = land2coords(landmarks)\n",
    "        return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "face_land = []\n",
    "for path in df_paths['paths']:\n",
    "    face_land.append(img_to_landmarks(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(result['paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_landmarks = pd.DataFrame.from_dict({'lmarks':face_land})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_landmarks.to_pickle(\"face_marks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_new = pd.read_pickle(\"face_marks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_new['lmarks'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##Join \"result\" & \"df_new\" to get emotions with facial landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_new.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pip  \n",
    "pip.main(['install', 'imutils'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import skvideo.io\n",
    "import skvideo.datasets\n",
    "videodata = skvideo.io.vread(skvideo.datasets.bigbuckbunny())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} sk-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_new['lmarks'][0]\n",
    "emomap = pd.read_csv(\"Cohn-Kanade Database FACS codes_v2.1.csv\")\n",
    "emomap['Emotion'].shape\n",
    "df_new['lmarks'].shape[0]\n",
    "scaler.fit_transform(df_new['lmarks'][i]).shape\n",
    "df_new['lmarks'][i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "#df_new['lmarks_scaled'] = []\n",
    "y_emo =[]\n",
    "for i in range(df_new['lmarks'].shape[0]):\n",
    "    df_new['lmarks'][i]= scaler.fit_transform(df_new['lmarks'][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor2(img_path):\n",
    "    img = image.load_img(img_path,grayscale=True, target_size=(48, 48))\n",
    "    x = image.img_to_array(img)\n",
    "    x = x[:, :, 0]\n",
    "    #x.transpose(2,0,1).reshape(3,-1)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor2(img_paths):\n",
    "    list_of_tensors = [path_to_tensor2(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#path_to_tensor2(result['paths'][0]).shape\n",
    "img = image.load_img(result['paths'][0],grayscale=True, target_size=(48, 48))\n",
    "x = image.img_to_array(img)\n",
    "x = x[:, :, 0]\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "X_tensors = paths_to_tensor2(result['paths']).astype('float32')/255\n",
    "#X_lstm = paths_to_tensor(result['paths']).astype('float32')/255\n",
    "\n",
    "X_tensors.shape\n",
    "#valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "#test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor2(img_path):\n",
    "    img = image.load_img(img_path,grayscale=True, target_size=(48, 48))\n",
    "    x = image.img_to_array(img)\n",
    "    x = x[:, :, 0]\n",
    "    #x.transpose(2,0,1).reshape(3,-1)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor2(img_paths):\n",
    "    list_of_tensors = [path_to_tensor2(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result['paths'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#y_catg = np_utils.to_categorical(np.array(result['Emotion']), 6)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "y_emo = result['Emotion'].groupby(result['seq']).tail(1).reset_index()\n",
    "\n",
    "lb_emo = LabelEncoder()\n",
    "y_emo_lstm = lb_emo.fit_transform(result['Emotion'])\n",
    "y_emo_lstm = pd.DataFrame.from_dict({'emo_code':y_emo_lstm})\n",
    "y_emo_codes = lb_emo.fit_transform(y_emo['Emotion'])\n",
    "y_emo_codes = pd.DataFrame.from_dict({'emo_code':y_emo_codes})\n",
    "ohe = OneHotEncoder()\n",
    "#y_emo_codes\n",
    "y_emo_oh = ohe.fit_transform(y_emo_codes)\n",
    "y_emo_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensors,y_emo_lstm['emo_code'], test_size=0.33, random_state=42, shuffle = False)\n",
    "X_train[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "model= Sequential()\n",
    "model.add(LSTM(128,input_shape = (48, 48) ,activation = 'relu', return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128,activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(6, activation = 'softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#opt = optimizers.Adam(lr = 1e-3, decay = 1e-5)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer = \"Adam\", metrics = ['accuracy'])\n",
    "model.fit(X_train,y_train, epochs = 10, validation_data = (X_test,y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "root = (\"cohn-kanade\")\n",
    "filepaths = []\n",
    "folders = []\n",
    "seq_length = 10\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        filepaths.append(os.path.join(path, name))\n",
    "        folders.append(path)\n",
    "        #print (os.path.join(path, name))\n",
    "df_paths = pd.DataFrame.from_dict({'paths':filepaths,'seq':folders})\n",
    "df_paths['key'] = df_paths.paths.str[:20]\n",
    "#df_paths\n",
    "emomap = pd.read_csv(\"Cohn-Kanade Database FACS codes_v2.1.csv\")\n",
    "result = pd.merge(df_paths, emomap, on='key', how='inner')\n",
    "#result['paths'][0]\n",
    "#result['paths']\n",
    "result2 = result.groupby('seq').tail(seq_length)\n",
    "seq_10 = result2.groupby('seq').size().reset_index(name='counts')\n",
    "seq_10 = seq_10.loc[seq_10['counts']==seq_length]\n",
    "\n",
    "result = pd.merge(result2, seq_10, on='seq', how='inner')\n",
    "result.shape\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "face_land = []\n",
    "for path in df_paths['paths']:\n",
    "    face_land.append(img_to_landmarks(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "#X_tensors = paths_to_tensor2(result['paths']).astype('float32')/255\n",
    "X_lstm = paths_to_tensor(result['paths']).astype('float32')/255\n",
    "X_lstm = X_lstm.reshape(451,5,48,48,1)\n",
    "X_lstm.shape\n",
    "#valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "#test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "y_emo = result['Emotion'].groupby(result['seq']).tail(1).reset_index()\n",
    "\n",
    "lb_emo = LabelEncoder()\n",
    "\n",
    "y_emo_codes = lb_emo.fit_transform(y_emo['Emotion'])\n",
    "y_emo_codes = pd.DataFrame.from_dict({'emo_code':y_emo_codes})\n",
    "ohe = OneHotEncoder()\n",
    "#y_emo_codes\n",
    "y_emo_oh = ohe.fit_transform(y_emo_codes).toarray()\n",
    "y_emo_codes\n",
    "#y_emo_oh = np_utils.to_categorical(np.array(y_emo_codes), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm,y_emo_codes['emo_code'], test_size=0.3, random_state=42, shuffle = True)\n",
    "y_train[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, ConvLSTM2D, BatchNormalization, Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=10, kernel_size=(3,3), activation='tanh',\n",
    "                             input_shape=[5] + list((48,48)) + [1],\n",
    "                             data_format='channels_last', return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(ConvLSTM2D(filters=10, kernel_size=(3,3), activation='tanh',\n",
    "                             input_shape=(5, 1) + (48,48,1),\n",
    "                             data_format='channels_last', return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(ConvLSTM2D(filters=10, kernel_size=(3,3), activation='tanh'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(ConvLSTM2D(filters=10, kernel_size=(4,4), activation='sigmoid'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=4, kernel_size=(3,3), activation=\"relu\", data_format=\"channels_last\"))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(units=100, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6,activation='softmax',name=\"Output\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#model.fit(X_train,y_train, batch_size=10, epochs=5, validation_data=(X_test,y_test))\n",
    "                       #callbacks=[ReduceLROnPlateau(), EarlyStopping(patience=3)])\n",
    "\n",
    "    \n",
    "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "epochs = 10\n",
    "\n",
    "csv_logger = CSVLogger('logs/training.csv')\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.ConvLSTM2D.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hist = model.fit(X_train,y_train, \n",
    "          validation_data=(X_test,y_test),\n",
    "          epochs=epochs, batch_size=10, callbacks=[checkpointer,csv_logger], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, ConvLSTM2D, BatchNormalization, Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sequential flows in parallel - cnn+face landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7334, 7)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "root = (\"cohn-kanade\")\n",
    "filepaths = []\n",
    "folders = []\n",
    "seq_length = 10\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        filepaths.append(os.path.join(path, name))\n",
    "        folders.append(path)\n",
    "        #print (os.path.join(path, name))\n",
    "df_paths = pd.DataFrame.from_dict({'paths':filepaths,'seq':folders})\n",
    "df_paths['key'] = df_paths.paths.str[:20]\n",
    "#df_paths\n",
    "emomap = pd.read_csv(\"Cohn-Kanade Database FACS codes_v2.2.csv\")\n",
    "result = pd.merge(df_paths, emomap, on='key', how='inner')\n",
    "#result['paths'][0]\n",
    "#result['paths']\n",
    "result2 = result.groupby('seq').tail(seq_length)\n",
    "seq_10 = result2.groupby('seq').size().reset_index(name='counts')\n",
    "seq_10 = seq_10.loc[seq_10['counts']==seq_length]\n",
    "\n",
    "#result = pd.merge(result2, seq_10, on='seq', how='inner')\n",
    "result.shape\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image                  \n",
    "#img_path = 'test_images/mark.jpg'\n",
    "def img_to_landmarks(imgpath):\n",
    "    img = cv2.imread(imgpath)\n",
    "    frame = imutils.resize(img, width=800)\n",
    "    frame_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    face_boundaries = face_detector(frame_gray,0)\n",
    "    for (enum,face) in enumerate(face_boundaries):\n",
    "        x = face.left()\n",
    "        y = face.top()\n",
    "        w = face.right() - x\n",
    "        h = face.bottom() - y\n",
    "        #cv2.rectangle(frame, (x,y), (x+w, y+h), (120,160,230),2)\n",
    "        landmarks = landmark_predictor(frame_gray, face)\n",
    "        landmarks = land2coords(landmarks)\n",
    "        return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import dlib\n",
    "landmark_predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "def land2coords(landmarks, dtype=\"int\"):\n",
    "    # initialize the list of tuples\n",
    "    # (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    " \n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (a, b)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
    " \n",
    "    # return the list of (a, b)-coordinates\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "#from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "face_land = []\n",
    "for path in result['paths']:\n",
    "    face_land.append(img_to_landmarks(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_landmarks = pd.DataFrame.from_dict({'lmarks':face_land})\n",
    "df_landmarks.to_pickle(\"face_landmarks_marks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_landmarks = pd.read_pickle(\"face_landmarks_marks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#face_land = np.array(face_land)\n",
    "#df_landmarks['lmarks'].shape\n",
    "x_lmark = np.vstack(df_landmarks['lmarks'])\n",
    "x_lmark = np.reshape(x_lmark,(7334,68,2))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "x_lmark = np.array([scaler.fit_transform(x) for x in x_lmark[:]])\n",
    "#df_landmarks['lmarks'][0]\n",
    "#x_lmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_landmarks_fer_train['lmarks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path,grayscale=True, target_size=(48, 48))\n",
    "    x = image.img_to_array(img)\n",
    "    #x = x[:, :, 0]\n",
    "    #x.transpose(2,0,1).reshape(3,-1)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "X_tensors = paths_to_tensor(result['paths']).astype('float32')/255\n",
    "#X_lstm = paths_to_tensor(result['paths']).astype('float32')/255\n",
    "\n",
    "X_tensors.shape\n",
    "#valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "#test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result['Emotion'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anger', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_catg = np_utils.to_categorical(np.array(result['Emotion']), 6)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.utils import np_utils\n",
    "#y_emo = result['Emotion'].groupby(result['seq']).tail(1).reset_index()\n",
    "\n",
    "lb_emo = LabelEncoder()\n",
    "y_emo_lstm = lb_emo.fit_transform(result['Emotion'])\n",
    "y_emo_lstm = pd.DataFrame.from_dict({'emo_code':y_emo_lstm})\n",
    "#y_emo_codes = lb_emo.fit_transform(y_emo_lstm['emo_code'])\n",
    "#y_emo_codes = pd.DataFrame.from_dict({'emo_code':y_emo_codes})\n",
    "ohe = OneHotEncoder()\n",
    "#y_emo_codes\n",
    "y_emo_oh = ohe.fit_transform(y_emo_lstm)\n",
    "\n",
    "emo_targets = np_utils.to_categorical(np.array(y_emo_lstm['emo_code']), 7)\n",
    "lb_emo.get_params(result['Emotion'])\n",
    "list(lb_emo.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emo_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, y_train, y_test = train_test_split(X_tensors,emo_targets, test_size=0.33, random_state=42, shuffle = True)\n",
    "X2_train, X2_test, y_train, y_test = train_test_split(x_lmark,emo_targets, test_size=0.33, random_state=42, shuffle = True)\n",
    "X2_train[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_lmark.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D,GlobalMaxPooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization, Merge\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import TruncatedNormal,glorot_normal\n",
    "import math\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "n = 0.01\n",
    "model.add(Conv2D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu',input_shape=(48, 48, 1)\n",
    "                 ,kernel_initializer=glorot_normal(seed=0)\n",
    "                 ,bias_initializer='zeros',name = \"Input\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling2D(pool_size=3, strides=2))\n",
    "\n",
    "n = 0.01\n",
    "model.add(Conv2D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu'\n",
    "                 ,kernel_initializer=glorot_normal(seed=0)\n",
    "                 ,bias_initializer='zeros'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling2D(pool_size=3, strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=4, strides=1, padding='same', activation='relu'\n",
    "                 ,kernel_initializer=glorot_normal(seed=0)\n",
    "                 ,bias_initializer='zeros'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "\n",
    "#model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024,activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(40,activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(15,activation = 'relu'))\n",
    "#n = math.sqrt(2.0/(9*256))\n",
    "model.add(Flatten())\n",
    "\n",
    "model_lmark = Sequential()\n",
    "model_lmark.add(Dense(1024,activation = 'relu',input_shape=(68,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model_lmark.add(Dense(40,activation = 'relu'))\n",
    "model_lmark.add(Flatten())\n",
    "\n",
    "\n",
    "\n",
    "merged = Sequential()\n",
    "merged.add(Merge([model, model_lmark], mode= 'concat'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "merged.add(Dense(7,activation='softmax',name=\"Output\"))\n",
    "\n",
    "merged.summary()\n",
    "merged.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#model.fit([X_tensors,x_lmark],y_emo_lstm['emo_code'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#merged.fit([X_tensors,x_lmark],y_emo_lstm['emo_code'])\n",
    "#merged.fit([X_tensors,x_lmark],emo_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#merged.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "epochs = 30\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_merged.csv')\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.merged.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hist = merged.fit([X1_train,X2_train],y_train, \n",
    "          validation_data=([X1_test,X2_test],y_test),\n",
    "          epochs=epochs, batch_size=100, callbacks=[checkpointer,csv_logger], verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged.load_weights('saved_models/weights.best.merged.hdf5')\n",
    "#merged.save('saved_models/merged_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "merged.save_weights('saved_models/merged_weights.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('saved_models/merged_architecture.json', 'w') as f:\n",
    "    f.write(merged.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\42\\envs\\emo\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\42\\envs\\emo\\lib\\site-packages\\keras\\engine\\topology.py:1206: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "with open('saved_models/merged_architecture.json', 'r') as f:\n",
    "    modelx = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "modelx.load_weights('saved_models/merged_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import dlib\n",
    "landmark_predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "def land2coords(landmarks, dtype=\"int\"):\n",
    "    # initialize the list of tuples\n",
    "    # (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    " \n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (a, b)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
    " \n",
    "    # return the list of (a, b)-coordinates\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_image = 'test_images\\Alan_Greenspan_0003.jpg'\n",
    "\n",
    "test_tensor = path_to_tensor(test_image).astype('float32')/255\n",
    "test_lmark = img_to_landmarks(test_image)\n",
    "test_lmark = scaler.fit_transform(test_lmark)\n",
    "#result['paths']\n",
    "test_lmark = np.expand_dims(test_lmark, axis=0)\n",
    "test_lmark.shape\n",
    "modelx.predict([test_tensor,test_lmark])\n",
    "list(lb_emo.classes_)[np.argmax(modelx.predict([test_tensor,test_lmark]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "def face_to_emo(frm):\n",
    "    img = cv2.resize(frm, (48,48), interpolation = cv2.INTER_AREA)\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    x = image.img_to_array(gray_image)\n",
    "    #res = np.concatenate([x[np.newaxis]])\n",
    "    x1 = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    x2 = face_to_landmarks(frm)\n",
    "    x2 = scaler.fit_transform(x2)\n",
    "    x2 = np.expand_dims(x2, axis=0)\n",
    "    return list(lb_emo.classes_)[np.argmax(modelx.predict([x1,x2]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "#img_path = 'test_images/mark.jpg'\n",
    "def face_to_landmarks(frm):\n",
    "    #img = cv2.imread(imgpath)\n",
    "    #frm = imutils.resize(frm, width=800)\n",
    "    frame_gray = cv2.cvtColor(frm,cv2.COLOR_BGR2GRAY)\n",
    "    face_boundaries = face_detector(frame_gray,0)\n",
    "    for (enum,face) in enumerate(face_boundaries):\n",
    "        x = face.left()\n",
    "        y = face.top()\n",
    "        w = face.right() - x\n",
    "        h = face.bottom() - y\n",
    "        #cv2.rectangle(frame, (x,y), (x+w, y+h), (120,160,230),2)\n",
    "        landmarks = landmark_predictor(frame_gray, face)\n",
    "        landmarks = land2coords(landmarks)\n",
    "        return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Alan_Greenspan_0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "import matplotlib.pyplot as plt\n",
    "#print(scaler.fit_transform(landmarks))\n",
    "#plt.plot([1,2,3,4])\n",
    "\n",
    "#plt.plot(df_new['lmarks'][1])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(486, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_catg = np_utils.to_categorical(np.array(result['Emotion']), 6)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "y_emo = result['Emotion'].groupby(result['seq']).tail(1).reset_index()\n",
    "\n",
    "lb_emo = LabelEncoder()\n",
    "y_emo_lstm = lb_emo.fit_transform(result['Emotion'])\n",
    "y_emo_lstm = pd.DataFrame.from_dict({'emo_code':y_emo_lstm})\n",
    "y_emo_codes = lb_emo.fit_transform(y_emo['Emotion'])\n",
    "y_emo_codes = pd.DataFrame.from_dict({'emo_code':y_emo_codes})\n",
    "ohe = OneHotEncoder()\n",
    "#y_emo_codes\n",
    "y_emo_oh = ohe.fit_transform(y_emo_codes)\n",
    "y_emo_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\42\\envs\\emo\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "    #import cv2\n",
    "    #from imutils import face_utils\n",
    "    import numpy as np\n",
    "    import argparse\n",
    "    import imutils\n",
    "    import dlib\n",
    "    import cv2\n",
    "    filepath = 'C:/Work/1. Coursera/4. MLND/machine-learning-master/machine-learning-master/projects/dog-project-master/to/emoticon'\n",
    "    vid = cv2.VideoCapture('test_vids/EdwardSnowden.mp4')\n",
    "    #vid = cv2.VideoCapture(filepath + 'Test.mp4')\n",
    "    #vid = cv2.VideoCapture(0)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    #out = cv2.VideoWriter('output.avi', fourcc, 20.0, (600,800))\n",
    "    count = 0\n",
    "    res_probs = []\n",
    "    res_face = []\n",
    "    frm_count = []\n",
    "            \n",
    "    if (vid.isOpened()== False): \n",
    "      print(\"Error opening video stream or file\")\n",
    "\n",
    "    while vid.isOpened():\n",
    "        ret,frame = vid.read()\n",
    "        if ret == True:\n",
    "            # resizing frame\n",
    "            # you can use cv2.resize but I recommend imutils because its easy to use\n",
    "            frame = imutils.resize(frame, width=800)\n",
    "\n",
    "            # grayscale conversion of image because it is computationally efficient\n",
    "            # to perform operations on single channeled (grayscale) image\n",
    "            frame_gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # detecting faces\n",
    "            face_boundaries = face_detector(frame_gray,0)\n",
    "            for (enum,face) in enumerate(face_boundaries):\n",
    "                # let's first draw a rectangle on the face portion of image\n",
    "                x = face.left()\n",
    "                y = face.top()\n",
    "                w = face.right() - x\n",
    "                h = face.bottom() - y\n",
    "                # Drawing Rectangle on face part\n",
    "                #cv2.rectangle(frame, (x,y), (x+w, y+h), (120,160,230),2)\n",
    "\n",
    "                # Now when we have our ROI(face area) let's\n",
    "                # predict and draw landmarks\n",
    "                landmarks = landmark_predictor(frame_gray, face)\n",
    "                # converting co-ordinates to NumPy array\n",
    "                landmarks = land2coords(landmarks)\n",
    "                x2 = np.expand_dims(scaler.fit_transform(landmarks), axis=0)\n",
    "\n",
    "                face = cv2.resize(frame, (48,48), interpolation = cv2.INTER_AREA)\n",
    "                gray_face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "                x1 = image.img_to_array(gray_face)\n",
    "                x1 = np.expand_dims(x1, axis=0)\n",
    "\n",
    "                for (a,b) in landmarks:\n",
    "                    # Drawing points on face\n",
    "                    cv2.circle(frame, (a, b), 2, (255, 0, 0), -1)\n",
    "                try:\n",
    "                    res_probs.append(modelx.predict([x1,x2])[0])\n",
    "                    #print(modelx.predict([x1,x2])[0])\n",
    "                    res_face.append(enum)\n",
    "                    frm_count.append(count)\n",
    "                    emoz = list(lb_emo.classes_)[np.argmax(modelx.predict([x1,x2]))] #face_to_emo(frame[y:y+h, x:x+w])\n",
    "                    max1 = max(modelx.predict([x1,x2]))\n",
    "                    cv2.rectangle(frame, (x,y), (x+w, y+h), (120,160,230),2)\n",
    "                    #print(np.argmax(modelx.predict([x1,x2])))\n",
    "                    cv2.putText(frame, \"Feeling :\" + str(emoz), (x - 10, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 128), 2)\n",
    "                    #cv2.putText(frame, \"Feeling :\" + str(max1), (20, 20+enum*20),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 128), 2)\n",
    "                except ValueError:\n",
    "                    cv2.putText(frame, \"Scanning..\" , (x - 10, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 128), 2)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            #out.write(frame)\n",
    "            cv2.imwrite(\"out_frames/frame%d.jpg\" % count, frame)\n",
    "            cv2.imshow(\"frame\", frame)\n",
    "\n",
    "            #print(frame.shape)\n",
    "            #  Stop if 'q' is pressed\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                break;\n",
    "            count += 1\n",
    "    vid.release()\n",
    "    \n",
    "    #out.release()\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#emo_df = pd.DataFrame(frm_count,res_face,res_probs, columns=['Frame','face','probs'])\n",
    "#print(res_probs)\n",
    "df = pd.DataFrame(res_probs,columns=list(lb_emo.classes_))\n",
    "df['Frames'] = frm_count\n",
    "df['Face'] = res_face\n",
    "df.to_csv('Snowden_Interview.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
